% This file should be replaced with your file with thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav Křena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Introduction}\label{ch:introduction}

The rapid advancement and widespread deployment of large language models (LLMs) have transformed natural-language interaction with computers.
These models now power chatbots, code assistants, translation systems, and creative tools used daily by millions of users.

However, their remarkable capabilities come with significant safety and ethical risks.
LLMs can generate harmful, biased, misleading, or illegal content when subjected to carefully crafted adversarial inputs, a practice commonly known as \emph{jailbreaking}~\cite{wei2023jailbroken, zou2023universal}.

Real-world incidents such as ChatGPT being tricked into providing bomb-making instructions~\cite{yaser2023potential}, or Gemini's image-generation controversy~\cite{shaw2025agonistic}, have demonstrated that even flagship commercial models remain vulnerable to adversarial prompting.
In response, red-teaming, a cybersecurity technique involving simulated attacks to expose vulnerabilities, has been adopted by leading AI organisations (OpenAI, Anthropic, Google DeepMind) as a core component of LLM safety evaluation~\cite{openai2023redteaming}.

Beyond model-level behaviour, LLMs are embedded in applications and services where integration and deployment issues create additional, system-level risks.
Industry guidance such as the OWASP Top 10 for Large Language Model Applications highlights a set of practical deployment vulnerabilities — for example prompt injection, insecure output handling, insecure plugin design, and excessive agency — that are distinct from but complementary to model-centred harms~\cite{owasp2025llm, owasp2025top}.

With the adoption of the EU AI Act in 2024, systematic risk assessment including red-teaming will become a legal requirement for high-risk AI systems deployed in the European Union from 2026 onward~\cite{eu2024aiact}.
Consequently, efficient, reproducible, and extensible red-teaming tools are no longer a luxury but an essential part of responsible AI development.

Despite significant progress, most existing open-source red-teaming frameworks suffer from limited modularity, poor support for modern systems, query cost and computational requirements that hinder adoption by smaller research teams and individual developers~\cite{purpura2025building, schoepf2025madmax, belaire2025automatic, munoz2024pyrit}.
This creates a clear need for a new, lightweight, developer-friendly red-teaming toolkit that lowers the barrier to LLM systematic safety testing.

The main goal of this bachelor's thesis is therefore the design, implementation, and evaluation of a modular open-source red-teaming toolkit for large language models that addresses the identified shortcomings of current solutions.

The remainder of this thesis is structured as follows:
Chapter~\ref{ch:background} surveys the current state of research in LLM red-teaming and reviews existing tools.
Chapter~\ref{ch:design} presents the proposed system architecture.
Chapter~\ref{ch:implementation} details the implementation of the red-teaming toolkit.
Chapter~\ref{ch:evaluation} evaluates the toolkit on selected LLMs.
Finally, Chapter~\ref{ch:conclusion} summarises the results and outlines directions for future work.

\chapter{Background and Related Work}\label{ch:background}

This chapter provides the technical and empirical background necessary to understand red-teaming of large language models (LLMs).

We first cover the architecture and training paradigm of modern LLMs and the alignment methods applied to improve their safety.

Next, we survey the primary categories of safety risks and known vulnerability classes.

We then turn to red-teaming methodology as adapted to LLMs — definitions, goals, and a taxonomy of test methods.

Subsequently, we describe common attack techniques (single-turn, multi-turn, prompt injection, universal triggers, trojans/backdoors)

Finally, we review evaluation metrics and benchmarks used to measure safety failures, and survey existing open-source tools and frameworks.

The chapter concludes with a synthesis of gaps in current tooling and evaluation practice, motivating the design of the toolkit proposed in this thesis.

\section{Large Language Models}\label{sec:llms}

Large language models (LLMs) are sequence-to-sequence (encoder-decoder) or autoregressive (decoder) models based on the Transformer architecture~\cite{vaswani2017attention}.

They are trained on massive, internet-scale corpora using a self-supervised objective such as next-token prediction.
After pretraining, many high-performance models undergo instruction fine-tuning and additional alignment to become more useful and safer in downstream use~\cite{zhang2024scaling}.

By 2025, publicly available and research-grade LLMs often exceed tens to hundreds of billions of parameters~\cite{lu2024blending}, and many incorporate advanced architectural and inference-optimisation techniques such as mixture-of-experts layers~\cite{fedus2022switch}, retrieval-augmented generation~\cite{borgeaud2022improving}, or quantisation-aware training for efficient deployment~\cite{dettmers2023qlora}.

Because of the scale of their training data and capacity, LLMs encode a wide variety of linguistic patterns, factual knowledge, biases, and behavioural priors.

This scale gives them impressive generative and reasoning capabilities, but also makes them susceptible to emergent, unintended behaviours that are not trivially predictable — including safety failures, policy evasion, and social-engineering style manipulation~\cite{wei2023jailbroken}.

\subsection{Alignment and Safety Interventions}\label{sec:alignment}

To mitigate risks from raw pre-trained models, developers commonly apply a pipeline of alignment techniques.

First, supervised fine-tuning (SFT) on curated instruction-following datasets helps the model adhere to desired task formats~\cite{zhang2024scaling, stiennon2022learning}.

Subsequently, reinforcement learning from human feedback (RLHF) is often used: human annotators rate model outputs, a reward model is trained on those ratings, and a policy optimisation algorithm such as Proximal Policy Optimisation (PPO) updates the model to align it to human preferences~\cite{ouyang2022training, schulman2017proximal}.

Variants such as Direct Preference Optimisation (DPO) or RLAIF (reinforcement learning from AI feedback) aim to improve efficiency and scalability of alignment without sacrificing safety or quality~\cite{rafailov2024direct, lee2024rlaif}.

Despite alignment efforts, evidence demonstrates that safety training is not foolproof.
Behavioural failures persist, especially under adversarial or adversary-chosen inputs: prompt-based attacks and jailbreaks remain a major vulnerability class (cf.\ \cite{wei2023jailbroken, zou2023universal, perez2022ignore}).

The fact that surface-level filtering and instruction tuning can be bypassed indicates that LLMs continue to rely on shallow heuristics rather than robust semantic safety guarantees~\cite{wei2023jailbroken, zou2023universal, perez2022ignore}.

\section{LLM Safety Risks}\label{sec:risk-taxonomy}

LLM misuse and unintended outputs pose a broad array of risks.

The following taxonomy, commonly adopted in recent red-teaming efforts~\cite{purpura2025building, mazeika2024harmbench}, captures the primary threat vectors:

\begin{itemize}
  \item \textbf{Malicious use:} generation of instructions or actionable content facilitating wrongdoing (e.g., construction of weapons or explosives, synthesis of illicit substances, or cyber intrusion).
  \item \textbf{Harassment, hate, and discrimination:} outputs that demean, harass or promote bias against individuals or protected groups.
  \item \textbf{Misinformation:} harmful or misleading claims that may influence user behaviour or beliefs.
  \item \textbf{Hallucinations:} fabricated or unfounded statements presented with confidence.
  \item \textbf{Privacy violations:} unintended leakage of sensitive information, either from memorised training data or through malicious prompts.
  \item \textbf{Self-harm / dangerous content:} generation of content promoting self-harm, suicide, or exploitation (especially of minors).
  \item \textbf{Emergent misuse and behavioural failures:} including instruction-following failures, refusal evasion (the model ignoring safety instructions), social-engineering exploitation, or covert manipulation over multiple turns.
  \item \textbf{Tool misuse / unsafe actions:} harmful commands or unintended actions when LLMs interface with external tools (e.g., code execution, browser control, or file manipulation).
\end{itemize}

These categories correspond to those used in large-scale safety benchmarks and red-team evaluation suites.

For example, frameworks such as {\em HarmBench} operationalise a broad set of harm categories and provide standard test sets and evaluation protocols across LLMs and red-teaming methods~\cite{mazeika2024harmbench}.

\subsection{Application- and System-level Risks: OWASP LLM Top 10 (2025)}\label{sec:owasp}

Beyond model-level behavioural vulnerabilities, real-world LLM deployments introduce additional system and integration risks.
The OWASP LLM Top 10 (2025)~\cite{owasp2025top} provides an industry-oriented overview of common failure modes encountered in practical LLM-based systems.
For completeness, the full list is provided below, together with indicative categories reflecting the level at which each risk typically manifests:

\begin{itemize}
    \item \textbf{LLM01: Prompt Injection} — crafted inputs override or manipulate system instructions. \emph{(Model-level)}
    \item \textbf{LLM02: Sensitive Information Disclosure} — unintended leakage of private or memorised data. \emph{(Model-level)}
    \item \textbf{LLM03: Supply Chain Vulnerabilities} — risks arising from compromised datasets, model weights, or third-party components. \emph{(Deployment-level)}
    \item \textbf{LLM04: Data and Model Poisoning} — malicious training or fine-tuning data influencing model behaviour. \emph{(Training-level)}
    \item \textbf{LLM05: Improper Output Handling} — insufficient sanitisation or validation of model outputs. \emph{(Application-level)}
    \item \textbf{LLM06: Excessive Agency} — autonomous or uncontrolled agent actions producing unintended effects. \emph{(Agent-level)}
    \item \textbf{LLM07: System Prompt Leakage} — extraction of hidden or system-level instructions. \emph{(Model-level)}
    \item \textbf{LLM08: Vector and Embedding Weaknesses} — vulnerabilities in retrieval-augmented generation (RAG) pipelines or embedding stores. \emph{(RAG-level)}
    \item \textbf{LLM09: Misinformation} — confident but incorrect or misleading outputs. \emph{(Model-level)}
    \item \textbf{LLM10: Unbounded Consumption} — excessive resource usage leading to cost escalation or denial-of-service. \emph{(Operational-level)}
\end{itemize}

\section{Red-teaming: Definitions and Methodology}\label{sec:red-teaming-def}

Originally developed in cybersecurity and military contexts, \emph{red-teaming} refers to the practice of simulating adversaries to probe system vulnerabilities before deployment~\cite{purpura2025building}.

In the context of LLMs, red-teaming denotes systematic probing of model behaviour via intentionally crafted adversarial prompts or inputs designed to circumvent safety and alignment measures, with the goal of discovering previously unknown failure modes, measuring their prevalence, and informing robust defences~\cite{schoepf2025madmax, belaire2025automatic}.

\newpage

Red-teaming campaigns in LLM settings typically have several objectives~\cite{purpura2025building, schoepf2025madmax}:

\begin{itemize}
  \item \textbf{Discovery:} reveal novel, previously undocumented failure modes (e.g., new jailbreak styles, multi-turn attack vectors, prompt injection in application contexts).  
  \item \textbf{Quantification:} estimate how often a model fails under adversarial conditions, enabling comparison across models and defence strategies.  
  \item \textbf{Reproducibility:} produce repeatable test cases and evaluation pipelines so that safety regressions can be detected over time.  
  \item \textbf{Defence hardening:} feed findings back into model tuning, safety filters, or deployment guardrails to reduce vulnerability.  
\end{itemize}

Unlike static benchmarks, red-teaming targets the worst-case behaviour of a model rather than its average-case performance, focusing on adversarial inputs specifically crafted to expose vulnerabilities~\cite{mazeika2024harmbench, schoepf2025madmax}.
Modern red-teaming approaches in LLMs span from fully manual adversarial testing to automated pipelines that integrate adversarial prompt generation, execution, and evaluation.

The following subsections describe manual, rule-based, and automated red-teaming methodologies in more detail.

\subsection{Manual, Rule-based and Automated Red-teaming}\label{sec:red-teaming-methods}

\begin{description}
  \item[Manual red-teaming:] human experts write adversarial prompts, simulate realistic misuse scenarios, and attempt to elicit harmful or policy-violating responses~\cite{purpura2025building}.
  This approach benefits from human creativity and insight into real-world misuse, including social-engineering, context-aware manipulation, and subtle or ambiguous scenarios.
  However, it is expensive, time-consuming, and often non-reproducible.

  \item[Rule-based / template-based testing:] uses curated prompt templates (e.g., standard jailbreaks, role-play prompts, obfuscation methods, encoded instructions) or transformation rules to generate adversarial inputs systematically~\cite{schoepf2025madmax, mazeika2024harmbench}.
  This method is reproducible and simple, but often limited to discovering known failure classes, and does not generalise to novel or adaptive attacks.

  \item[Automated red-teaming:] algorithmic or model-in-the-loop generation of adversarial prompts, for example via search, optimisation, or by using another LLM as the attacker~\cite{schoepf2025madmax, belaire2025automatic}.
  Notable work in this space includes {\em Tree of Attacks (TAP)} which automatically generates jailbreak prompts against black-box LLMs by iteratively refining candidates in a tree-structured search and pruning unlikely attack paths~\cite{mehrotra2024tap}.
  Automated methods scale well, explore large prompt spaces, and can discover novel failures, but may also generate unrealistic or trivial prompts, and often depend on the quality of the attacker or judge model and the underlying search strategy.
\end{description}

Automated red-teaming has recently been re-envisioned as a sequential, multi-turn process rather than isolated single-turn attempts.
For example, recent work models red-teaming as a Markov Decision Process (MDP), using hierarchical reinforcement learning (RL) to optimise long-horizon attacks over entire dialogue trajectories~\cite{belaire2025automatic}.

\section{Attack Taxonomy}\label{sec:attack-taxonomy}

Adversarial attacks against LLMs span a diverse space of techniques that exploit different aspects of model behaviour and prompting dynamics.
Organising these attacks into a taxonomy is useful for understanding their mechanisms, comparing red-teaming methods, and designing systematic evaluation pipelines~\cite{purpura2025building}.

This section surveys the primary attack classes considered in contemporary LLM red-teaming: single-turn jailbreaks, prompt-injection attacks, multi-turn conversational manipulation, universal or transferable triggers, and trojan or backdoor-style vulnerabilities~\cite{schoepf2025madmax}.

\subsection{Single-turn Jailbreaking}\label{sec:single-turn}

Single-turn jailbreaks are adversarial prompts supplied in one-shot (single user message) that instruct the model to ignore its safety filters or system instructions~\cite{wei2023jailbroken}.

Common techniques include role-play (``Pretend you are …''), direct overrides (``Ignore previous instructions …''), encoding or cipher-based obfuscation to hide disallowed instructions, and other prompt-engineering strategies~\cite{perez2022ignore}.

Despite alignment training, many models remain vulnerable to such simple attacks~\cite{wei2023jailbroken, zou2023universal}.

\subsection{Prompt Injection}\label{sec:prompt-injection}

Prompt injection refers to attacks where user-supplied input (or external content, in the case of integrated applications) is directly interpreted by the LLM as instructions, potentially overriding or modifying hidden system prompts~\cite{perez2022ignore}.
This is a major concern for real-world applications embedding LLMs (chatbots, agents, document processors, pipelines), because attackers can inject maliciously crafted instructions that the model interprets as legitimate.
Empirical studies have demonstrated that many deployed LLM-based applications are vulnerable to prompt injection — for example via the {\em HouYi} attack, which compromised dozens of applications in a black-box setting~\cite{liu2024promptinjection}.
Other work has shown that even automated, universal prompt injection attacks remain effective under defensive measures~\cite{liu2024automaticpi}.

Prompt injection remains among the most significant security threats for LLM-based systems, as acknowledged by security guidance frameworks and cheat sheets (e.g., from OWASP) tailored to LLM applications~\cite{owasp2025llm, owasp2025top}.

\subsection{Multi-turn and Conversational Attacks}\label{sec:multi-turn}

Rather than achieve a jailbreak in a single prompt, adversaries may perform a sequence of manipulative steps — gradually steering the conversation, exploiting context persistence, memory, and the model's inability to consistently refuse undesirable requests~\cite{wei2023jailbroken}.

Multi-turn attacks may involve context poisoning, social-engineering style dialogue, bait-and-switch tactics, or incremental obfuscation.
This vector is increasingly recognised as one of the most dangerous and under-evaluated, as multi-step interactions more closely resemble realistic misuse scenarios.

Recent research recasts automated red-teaming as a multi-turn optimisation problem — better capturing long-horizon adversarial strategies — and shows that RL-based red-teaming significantly outperforms single-turn methods in eliciting harmful content~\cite{belaire2025automatic}.

\subsection{Universal / Transferable Triggers and Trojan-style Attacks}\label{sec:universal}

Universal triggers or transferable adversarial prompts aim to find short token sequences (prefixes, suffixes, or embedded instructions) that reliably trigger undesired model behaviour across different inputs and even across different models~\cite{zou2023universal}.
This makes the attack highly reusable and dangerous.

One example is the black-box Trojan prompt attack framework {\em TrojLLM}, which demonstrates that universal stealthy triggers can be discovered for widely-used LLM architectures and APIs, enabling malicious manipulation across diverse prompt inputs~\cite{xue2023trojllm}.
Such attacks bypass input sanitisation heuristics because the trigger is embedded in seemingly benign inputs.

\subsection{Prompt-based Evasion and Adversarial Perturbations}\label{sec:perturbation}

Beyond explicit instructions or injected content, adversaries may attempt adversarial prompting via subtle token-level perturbations, insertions, deletions, or encodings — minimal but adversarial changes that remain semantically similar to benign prompts yet cause the model to deviate~\cite{zou2023universal}.

Defences based on sanitisation or heuristic filtering often fail to detect such manipulations, especially in the presence of context sensitivity or long prompts.
Certified-safety approaches have been proposed to mitigate this class: for example, erase-and-check, which systematically removes tokens and reruns safety filters to detect adversarial prompt manipulations, providing (under assumptions) a safety guarantee against insertions, suffixes, or adversarial infusions up to a bounded size~\cite{kumar2025certifying}.
While promising, these methods are computationally expensive and may degrade user experience or model utility.

Because these perturbation-based attacks operate at the token level and can be obfuscated, they represent a difficult-to-detect threat, especially when combined with other attack modalities (multi-turn, universal triggers, trojans).

\section{Evaluation Metrics and Benchmarks}\label{sec:evaluation}

Robust and systematic evaluation is critical for red-teaming, as it enables meaningful comparison of models, attack strategies, and defences.
The following metrics and benchmarks are widely adopted in contemporary LLM safety research and practice.

\subsection{Quantitative Metrics}\label{sec:metrics-quant}

\begin{itemize}
  \item \textbf{Attack Success Rate (ASR):} the proportion of adversarial attempts (prompts) that succeed in eliciting harmful or policy-violating outputs. ASR is the most common metric, but it depends heavily on the definition of "harmful" and on the quality of the judge (classifier, LLM-judge, human).  
  \item \textbf{Refusal Bypass Rate:} a variant of ASR that measures how often safe-mode refusals or safety filters are bypassed — i.e., the model produces a disallowed output rather than refusing the request.  
  \item \textbf{Judge Reliability Metrics:} when using automated judges (regex, classifier, LLM-based), it is crucial to measure false positives / false negatives, calibration error, and inter-annotator agreement (if doing human calibration). Poor judges can substantially distort ASR and other metrics.  
  \item \textbf{Robustness Metrics:} measure the stability of attacks or defences under input variations — such as paraphrasing, prompt reordering, model temperature differences, different seeds (randomness), context shuffling, or small perturbations.  
  \item \textbf{Transferability / Generality:} measure how well attacks discovered on one model or configuration transfer to other models, prompts, or deployments.  
  \item \textbf{Conversational / Trajectory Metrics:} in multi-turn attack scenarios, metrics may capture success over a dialogue trajectory (e.g., whether harmful content emerges at any point), time-to-failure, or complexity (number of turns required).  
\end{itemize}

\subsection{Benchmarks and Standardised Suites}\label{sec:benchmarks}

To enable systematic comparison across red-teaming methods and LLMs, several standard benchmarks have recently been developed.
A prominent example is {\em HarmBench}, which provides an open-source evaluation framework, a large pool of red-teaming methods and a diverse set of target models and defences~\cite{mazeika2024harmbench}.

HarmBench enables reproducible large-scale red-team evaluation and supports both attack- and defence-side experiments.
Its release marks a milestone toward standardising LLM safety evaluation pipelines.

Despite this progress, many prior works still rely on ad-hoc prompt sets, non-public prompt libraries, or private internal red-team pipelines.
In addition, while single-turn attack benchmarks are relatively common, multi-turn or conversational red-teaming remains underrepresented in publicly available benchmark suites.

\section{Open-source Red-Teaming Tools and Frameworks}\label{sec:tool-survey}

A variety of open-source frameworks and research prototypes have been developed to support systematic red-teaming of LLMs.
Their design choices, capabilities, and limitations differ significantly, reflecting diverse goals and use cases.

\subsection{Frameworks for rule-based or template-based red-teaming}\label{sec:pyrit-garak}

Frameworks such as {\em PyRIT} provide a modular architecture for plugging in model backends, defining prompt templates, logging results, and running interactive or batch red-teaming sessions.
These tools are often lightweight and well-suited for smaller projects or smaller compute budgets, but tend to lack advanced automated search or generation capabilities~\cite{munoz2024pyrit}.

Another widely used toolkit is {\em garak}.
garak provides "probes, generators, and detectors": probes manage attack logic; generators abstract target models (LLMs, dialog systems, or any component taking text and returning text); detectors assess whether output indicates a successful attack; and the framework compiles results into human-readable reports (HTML and JSON).
This design allows red-teaming across a variety of model backends and output modalities, but — as with other rule-based frameworks — it may not support automated, learning-based attack generation or multi-turn conversational scenarios out of the box~\cite{derczynski2024garak}.

\newpage{}

\subsection{Automated Red-Teaming Frameworks}\label{sec:madmax-automatic}

More advanced frameworks attempt to integrate attack generation, execution, and evaluation into a unified pipeline.
Notable recent work includes {\em MAD-MAX}, a modular adversarial red-teaming framework designed to allow multiple attack strategies (template-based, search-based, LLM-driven) within a pluggable architecture~\cite{schoepf2025madmax}.
Its modular nature makes it flexible and extensible, but in practice integrating it with diverse model runtimes (local LLaMA variants, API-based models, multi-modal models) and scaling up to large-scale red-teaming campaigns remains challenging due to compute demands, backend model compatibility, and evaluation infrastructure requirements.

Beyond modular frameworks, recent work explores fully automated, trajectory-based red-teaming that recasts red-teaming as a sequential decision-making process over entire dialogues.
For example, the automated red-teaming approach by Belaire et al. (2025) formalises multi-turn red-teaming as a Markov Decision Process (MDP), enabling attack policies to optimise over entire conversation trajectories rather than single messages~\cite{belaire2025automatic}.
This methodology captures realistic adversarial behaviour and reveals vulnerabilities that single-turn or template-based attacks may miss.

\subsection{Defence-oriented and Certified Safety Approaches}\label{sec:defences}

In response to the growing sophistication of attacks, some work focuses on hardening LLMs against adversarial prompting.

For example, the framework "erase-and-check" provides a method for certifying safety against adversarial prompt modifications (suffix insertion, insertion at arbitrary positions, adversarial infusions) under bounded adversarial size.

This method recomputes safety classification after systematically removing tokens from the prompt and offers provable safety guarantees under certain assumptions~\cite{kumar2025certifying}.

While promising, such approaches are often computationally expensive and may impair user experience or model usability.

Moreover, the existence of Trojan prompt attacks (as demonstrated by frameworks like {\em TrojLLM}) indicates that even seemingly benign prompts may embed stealthy triggers that cause harmful behaviour — which complicates defence strategies, necessitating robust input sanitisation, runtime monitoring, and possibly dynamic prompt sanitisation or adversarial-resistant prompt encoding~\cite{xue2023trojllm}.

\section{Synthesis and Research Gap}\label{sec:gaps}

The literature and tools surveyed above show that — while the research community has developed a rich taxonomy of harms, attack strategies, and evaluation metrics — substantial gaps remain that limit the effectiveness, accessibility, and reliability of red-teaming for large language models.

In particular:

\begin{itemize}
  \item \textbf{Lack of lightweight, modular, and extensible tooling:} existing frameworks either focus on small scale (template-based, rule-based, lightweight) or on large-scale automated red-teaming — but rarely both. There is a lack of toolkits that are accessible to small research teams or individual developers yet support modern LLM backends, extensible attack/evaluator plugins, and reproducible logging/benchmarks.  
  \item \textbf{Judge reliability and evaluation consistency:} many red-teaming efforts rely on heuristic or automated judges (regex filters, simple classifiers), which often lack calibration, robustness, or reproducibility. As a result, reported Attack Success Rates (ASR) may misrepresent true safety risk. While certified-safety methods (e.g., erase-and-check) provide stronger guarantees, they are computationally expensive and may be impractical for everyday red-teaming.  
  \item \textbf{Underrepresentation of multi-turn and real-world attack vectors:} most prior work and benchmarks focus on single-shot prompts; multi-turn conversational attacks — which more accurately model real-world adversaries — remain under-evaluated. Recently proposed trajectory-based red-teaming frameworks help, but are not yet part of standard open-source toolkits.  
  \item \textbf{Difficulty integrating across diverse model backends and deployment contexts:} LLMs are deployed in variable settings (local open-source models, API-based proprietary models, multi-modal agents, applications with external tools). Existing tools often lack abstractions or adapters covering this diversity.  
  \item \textbf{Limited transparency and reproducibility of prompt libraries and experiment artefacts:} many studies do not release their full prompt sets, seeds, or evaluation logs, making independent replication or longitudinal safety regression testing difficult.  
\end{itemize}

These gaps motivate the design goals for the toolkit developed in this thesis: namely, modularity and pluggability (attack generators, model adapters, evaluators), reproducible experiment manifests and logging, hybrid judging (to balance cost and fidelity), and explicit support for multi-turn conversational testing.

By addressing these gaps, the proposed toolkit aims to lower the barrier to entry for systematic LLM red-teaming for researchers, students, and small teams.

\section{Conclusion of the Chapter}\label{sec:chapter-conclusion}

In this chapter we surveyed the technical foundations of large language models — their architecture, alignment techniques, and associated safety risks — and we reviewed a broad spectrum of attack vectors: from simple single-shot jailbreaks to prompt injection, multi-turn manipulations, universal triggers, and Trojan-style prompt attacks.

We examined the diversity of red-teaming methodologies (manual, rule-based, and automated), and we discussed evaluation metrics and recent benchmark frameworks such as {\em HarmBench}.

We also surveyed existing open-source tools and frameworks, highlighting trade-offs between accessibility, scalability, and coverage.

Finally, we identified core gaps in tooling, evaluation, and reproducibility — gaps that motivate the design and implementation of the red-teaming toolkit proposed in the next chapter.

With this foundation in place, the next chapter will provide a detailed survey of existing open-source red-teaming toolkits, followed by a requirements analysis and the design of the proposed modular, extensible toolkit.

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabularx}{\linewidth}{l X X X X X}
\toprule
\textbf{Framework} &
\textbf{Attack Types Supported} &
\textbf{Model Backend Support} &
\textbf{Multi-turn Capability} &
\textbf{Evaluation / Judge Type} &
\textbf{Key Strengths / Limitations} \\
\midrule

\textbf{PyRIT} \cite{munoz2024pyrit} &
Rule-based prompts, template attacks, basic prompt injection, simple adversarial transforms. &
Supports API-based models (OpenAI, Anthropic), simple REST adapters; limited support for local LLMs. &
\textcolor{gray}{Partial} - supports manual conversational sessions but lacks automated multi-turn logic. &
Basic keyword/regex checks; optional external classifiers. &
+ Lightweight, easy to use.  
-- Limited automation, no search-based generators, weak judge reliability. \\

\midrule
\textbf{garak} \cite{derczynski2024garak} &
Template attacks, perturbation probes, obfuscation, prompt rewriting via "generators". &
API models + HuggingFace backends; modular "generators allow extension. &
\textcolor{gray}{Partial} - supports sequential probing, but not full conversational trajectories. &
Detectors: regex, classifiers; HTML/JSON reporting. &
+ Mature, widely used, structured probes.  
-- Limited advanced automated attack generation; multi-turn support minimal. \\

\midrule
\textbf{MAD-MAX} \cite{schoepf2025madmax} &
Modular system: rule-based, template-based, LLM-generated attacks, search-based attacks. &
Designed for flexible backends; supports API and local models; plugin-based adapters. &
\textbf{Yes} - modular conversation orchestrator; supports multi-turn flows. &
LLM-as-a-judge, rule-based detectors, modular evaluation pipeline. &
+ Highly modular, extensible.  
-- High compute requirements; complex to deploy; best suited for labs. \\

\midrule
\textbf{Automatic LLM Red-teaming} \\ (Belaire et al. 2025) \cite{belaire2025automatic} &
LLM-as-attacker, RL/HF-based optimisation, trajectory-level red-teaming, multi-turn adversarial search. &
API and local models; depends on model-in-the-loop capabilities. &
\textbf{Yes} - multi-turn attacks optimised using RL / MDP formulation. &
LLM judges (safety classifiers, preference models). &
+ State-of-the-art automated discovery; finds subtle failures.  
-- Expensive; requires careful calibration; not lightweight. \\

\midrule
\textbf{TAP (Tree-of-Attacks)} \cite{mehrotra2024tap} &
Search-based automated jailbreaks, adversarial refinement tree, black-box optimisation. &
Black-box API models (OpenAI, Anthropic, etc.). Local models possible with wrappers. &
\textcolor{gray}{No} - primarily single-turn; generates one-shot optimised attacks. &
Simple heuristic judges; optional LLM-as-a-judge extension. &
+ Extremely query-efficient; discovers novel jailbreaks.  
-- Limited to single-turn; no conversational capabilities; not a full toolkit. \\

\midrule
\textbf{Prompt Injection Frameworks} \\ (e.g., HouYi) \cite{liu2024promptinjection} &
Prompt injection, indirect injection, document-based attacks, automated crawling and exploitation. &
Targets LLM-integrated apps (not just raw models). &
\textbf{Yes} - multi-step workflows across web/app interactions. &
Task-specific checks; manual validation often required. &
+ Highly realistic; identifies real-world vulnerabilities.  
-- Not general red-teaming against LLMs alone; focus on apps. \\

\midrule
\textbf{Certified Safety Tools} \\ (Erase-and-Check) \cite{kumar2025certifying} &
Defence: adversarial prompt removal, token deletion checks, suffix attack detection. &
Model-agnostic; operates at prompt-processing stage. &
N/A (defence, not attack). &
Binary safety guarantee after checking. &
+ Formal safety guarantees for bounded adversarial prompts.  
-- Very slow; not suitable for large-scale red-teaming. \\

\bottomrule
\end{tabularx}
\caption{Comparison of major LLM red-teaming frameworks, covering supported attack types, backend compatibility, multi-turn capabilities, evaluation methods, and key strengths and limitations.}
\label{tab:framework-comparison}
\end{table}

\chapter{Design}\label{ch:design}

\chapter{Implementation}\label{ch:implementation}

\chapter{Evaluation}\label{ch:evaluation}

\chapter{Conclusion}\label{ch:conclusion}


% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}