% This file should be replaced with your file with thesis content.
%=========================================================================
% Authors: Michal Bidlo, Bohuslav Křena, Jaroslav Dytrych, Petr Veigend and Adam Herout 2019

% For compilation piecewise (see projekt.tex), it is necessary to uncomment it and change
% \documentclass[../projekt.tex]{subfiles}
% \begin{document}

\chapter{Introduction}\label{ch:introduction}

The rapid advancement and widespread deployment of large language models (LLMs) such as GPT-5.1, Claude Sonnet 4.5, Gemini 3 and others have transformed natural-language interaction with computers.
These models power chatbots, code assistants, translation services, and creative tools used daily by millions of users.

However, their remarkable capabilities come with significant safety and ethical risks.
LLMs can generate harmful, biased, misleading, or illegal content when subjected to carefully crafted adversarial inputs, a practice commonly known as \emph{jailbreaking}~\cite{wei2023jailbroken, zou2023universal}.

Real-world incidents such as ChatGPT being tricked into providing bomb-making instructions~\cite{yaser2023potential}, or Gemini's image-generation controversy~\cite{andrew2025agonistic}, have demonstrated that even flagship commercial models remain vulnerable.
In response, red teaming, a cybersecurity technique involving simulated attacks to expose vulnerabilities, has been adopted by leading AI organisations (OpenAI, Anthropic, Google DeepMind, Meta AI) as a core component of LLM safety evaluation~\cite{openai2024redteaming}.

With the adoption of the EU AI Act in 2024, systematic risk assessment including red teaming will become a legal requirement for high-risk AI systems deployed in the European Union from 2026 onward~\cite{eu2024aiact}.
Consequently, efficient, reproducible, and extensible red-teaming tools are no longer a luxury but an essential part of responsible AI development as it will be a legal requirement in the future.

Despite significant progress, most existing open-source red-teaming frameworks suffer from limited modularity, poor support for modern systems, query cost and computational requirements, etc., that hinder adoption by smaller research teams and individual developers~\cite{purpur2025building, schoepf2025madmax, belaire2025automatic, munoz2024pyrit}.
This creates a clear need for a new, lightweight, developer-friendly red-teaming toolkit that lowers the barrier to LLM systematic safety testing.

The main goal of this bachelor's thesis is therefore the design, implementation, and evaluation of a modular open-source red-teaming toolkit for large language models that addresses the identified shortcomings of current solutions.

\chapter{Background and Related Work}\label{ch:background}

This chapter provides the technical and empirical background necessary to understand red-teaming of large language models (LLMs). We first cover the architecture and training paradigm of modern LLMs and the alignment methods applied to improve their safety. Next, we survey the primary categories of safety risks and known vulnerability classes. We then turn to red-teaming methodology as adapted to LLMs — definitions, goals, and a taxonomy of test methods. Subsequently, we describe common attack techniques (single-turn, multi-turn, prompt injection, universal triggers, trojans/backdoors). Finally, we review evaluation metrics and benchmarks used to measure safety failures, and survey existing open-source tools and frameworks. The chapter concludes with a synthesis of gaps in current tooling and evaluation practice, motivating the design of the toolkit proposed in this thesis.

\section{Large Language Models}\label{sec:llms}

Large language models (LLMs) are sequence-to-sequence or autoregressive models based on the Transformer architecture \cite{vaswani2017attention}. They are trained on massive, internet-scale corpora using a self-supervised objective such as next-token prediction. After pretraining, many high-performance models undergo instruction fine-tuning and additional alignment to become more useful and safer in downstream use.

By 2025, publicly-available and research-grade LLMs often exceed tens to hundreds of billions of parameters, employing advanced architectural and optimization features (e.g., mixture-of-experts, retrieval augmentation, quantization-aware training). Because of the scale of their training data and capacity, LLMs encode a wide variety of linguistic patterns, factual knowledge, biases, and behavioural priors. This scale gives them impressive generative and reasoning capabilities, but also makes them susceptible to emergent, unintended behaviors that are not trivially predictable — including safety failures, policy evasion, and social-engineering style manipulation.

\subsection{Alignment and Safety Interventions}\label{sec:alignment}

To mitigate risks from raw pre-trained models, developers commonly apply a pipeline of alignment techniques. First, supervised fine-tuning (SFT) on curated instruction-following datasets helps the model adhere to desired task formats. Subsequently, reinforcement learning from human feedback (RLHF) is often used: human annotators rate model outputs, a reward model is trained on those ratings, and a policy optimization method (e.g., PPO) updates the model to align it to human preferences. Variants like Direct Preference Optimization (DPO) or RLAIF (reinforcement learning from AI feedback) aim to improve efficiency and scalability of alignment without sacrificing safety or quality.

Despite alignment efforts, evidence demonstrates that safety training is not foolproof. Behavioural failures persist, especially under adversarial or adversary-chosen inputs: prompt-based attacks and jailbreaks remain a major vulnerability class (cf.\ \cite{wei2023jailbroken, zou2023universal, perez2022ignore}). The fact that surface-level filtering and instruction tuning can be bypassed indicates that LLMs continue to rely on shallow heuristics rather than robust semantic safety guarantees.

\section{LLM Safety Risks}\label{sec:risk-taxonomy}

LLM misuse and unintended outputs pose a broad array of risks. The following taxonomy, commonly adopted in recent red-teaming efforts, captures the primary threat vectors:

\begin{itemize}
  \item \textbf{Malicious usage:} generation of instructions or actionable content facilitating wrongdoing (e.g., bomb-making, illicit substances, hacking).  
  \item \textbf{Harassment, hate, and discrimination:} outputs that demean or promote bias against individuals or protected groups.  
  \item \textbf{Misinformation and hallucinations:} confidently stated false information, invented facts, or fabricated references.  
  \item \textbf{Privacy violations:} unintended leakage of sensitive information, either from memorized training data or through malicious prompts.  
  \item \textbf{Self-harm / dangerous content:} generation of content promoting self-harm, suicide, or exploitation (especially of minors).  
  \item \textbf{Emergent misuse and behavioural failures:} including instruction-following failures, refusal evasion (the model ignoring safety instructions), social-engineering exploitation, or covert manipulation over multiple turns.  
\end{itemize}

These categories correspond to those used in large-scale safety benchmarks and red-team evaluation suites. For example, frameworks such as {\em HarmBench} operationalize a broad set of harm categories and provide standard test sets and evaluation protocols across LLMs and red-teaming methods \cite{mazeika2024harmbench}.

\section{Red Teaming: Definitions and Methodology}\label{sec:red-teaming-def}

Originally developed in cybersecurity and military contexts, \emph{red teaming} refers to the practice of simulating adversaries to probe system vulnerabilities before deployment. In the context of LLMs, red teaming denotes systematic probing of model behaviour via adversarial prompts or inputs designed to circumvent safety and alignment measures, with the goal of discovering previously unknown failure modes, measuring their prevalence, and informing robust defenses.

Red-team campaigns in LLM settings typically have several objectives:

\begin{itemize}
  \item \textbf{Discovery:} reveal novel, previously undocumented failure modes (e.g., new jailbreak styles, multi-turn attack vectors, prompt injection in application contexts).  
  \item \textbf{Quantification:} estimate how often a model fails under adversarial conditions, enabling comparison across models and defense strategies.  
  \item \textbf{Reproducibility:} produce repeatable test cases and evaluation pipelines so that safety regressions can be detected over time.  
  \item \textbf{Defense hardening:} feed findings back into model tuning, safety filters, or deployment guardrails to reduce vulnerability.  
\end{itemize}

Modern red-teaming approaches in LLMs span from fully manual human-driven pen-testing to automated pipelines combining generation, execution, and evaluation.

\subsection{Manual, Rule-Based and Automated Red Teaming}\label{sec:red-teaming-methods}

\begin{description}
  \item[Manual red teaming:] human experts write adversarial prompts, simulate realistic misuse scenarios, and attempt to elicit harmful or policy-violating responses. This approach benefits from human creativity and insight into real-world misuse, including social-engineering, context-aware manipulation, or subtle misuse cases. However, it is expensive, time-consuming, and often non-reproducible.

  \item[Rule-based / template-based testing:] uses curated prompt templates (e.g., standard jailbreaks, role-play prompts, obfuscation methods, encoded instructions) or transformation rules to generate adversarial inputs systematically. This method is reproducible and simple, but often limited to discovering known failure classes, and does not generalize to novel attacks.

  \item[Automated red teaming:] algorithmic or model-in-the-loop generation of adversarial prompts — e.g., via search, optimization, or by using another LLM as attacker. Notable work in this space includes {\em Tree of Attacks (TAP)} which automatically generates jailbreak prompts against black-box LLMs by iteratively refining candidates and pruning unlikely ones \cite{mehrotra2023tap}. Automated methods scale well, explore large prompt spaces, and can discover novel failures, but also risk producing unrealistic or trivial prompts — and often depend on the quality of the attacker/judge model and search strategy.
\end{description}

Automated red-teaming has recently been re-envisioned as a sequential, multi-turn process rather than isolated single-turn attempts. For example, recent work models red-teaming as a Markov Decision Process (MDP), using hierarchical reinforcement learning to optimize long-horizon attacks over entire dialogue trajectories \cite{belaire2025automatic}.

\section{Attack Taxonomy}\label{sec:attack-taxonomy}

Here we outline the main classes of attacks exploited in LLM red-teaming, including prompt-based attacks, injection attacks, multi-turn manipulations, universal triggers, and trojan/backdoor style vulnerabilities.

\subsection{Single-turn Jailbreaking}\label{sec:single-turn}

Single-turn jailbreaks are adversarial prompts supplied in one-shot (single user message) that instruct the model to ignore its safety filters or system instructions. Common techniques include role-play (``Pretend you are …''), direct overrides (``Ignore previous instructions …''), encoding or cipher-based obfuscation (to hide disallowed instructions), and other prompt-engineering tricks. Despite alignment training, many models remain vulnerable to such simple attacks \cite{wei2023jailbroken, zou2023universal}.

\subsection{Prompt Injection}\label{sec:prompt-injection}

Prompt injection refers to attacks where user-supplied input (or external content, in the case of integrated applications) is directly interpreted by the LLM as instructions, potentially overriding or modifying hidden system prompts. This is a major concern for real-world applications embedding LLMs (chatbots, agents, document processors, pipelines) because attackers can inject malicious instructions that the model treats as legitimate. Empirical studies have demonstrated that many deployed LLM-based applications are vulnerable to prompt injection — for example via the {\em HouYi} attack, which compromised dozens of applications in a black-box setting \cite{liu2023promptinjectionapps}. Other work has shown that even automated, universal prompt injection attacks remain effective under defensive measures \cite{liu2024automaticpi}.

Prompt injection remains among the most significant security threats for LLM-based systems, as acknowledged by security guidance frameworks and cheat sheets (e.g., from OWASP) tailored to LLM applications \cite{turn0search9}.

\subsection{Multi-turn and Conversational Attacks}\label{sec:multi-turn}

Rather than achieve a jailbreak in a single prompt, adversaries may perform a series of manipulative steps — gradually steering the conversation, exploiting context persistence, memory, and the model’s inability to consistently refuse undesirable requests. Multi-turn attacks may involve context poisoning, social-engineering style dialogue, bait-and-switch tactics, or incremental obfuscation. This vector is increasingly recognized as one of the most dangerous and under-evaluated. Recent research recasts automated red-teaming as a multi-turn optimization problem — which better reflects realistic adversarial scenarios — and shows that RL-based red-teaming significantly outperforms single-turn methods in eliciting harmful content \cite{belaire2025automatic}.

\subsection{Universal / Transferable Triggers and Trojan-style Attacks}\label{sec:universal}

Universal triggers or transferable adversarial prompts aim to find short token sequences (prefixes, suffixes, or embedded instructions) that reliably trigger undesired model behaviour across different inputs and even across different models. This makes the attack highly reusable and dangerous. One example is the black-box Trojan prompt attack framework {\em TrojLLM}, which demonstrates that universal stealthy triggers can be discovered for widely-used LLM APIs such as GPT-3.5 and GPT-4, enabling malicious manipulation across diverse prompt inputs \cite{xue2023trojllm}. Such attacks bypass input sanitization heuristics because the trigger is embedded in seemingly benign inputs.

\subsection{Prompt-based Evasion and Adversarial Perturbations}\label{sec:perturbation}

Beyond explicit instructions or injected content, adversaries may attempt adversarial prompting via subtle token-level perturbations, insertion, deletion, or encoding — i.e., minimal but adversarial changes that remain semantically similar to benign prompts but cause the model to deviate. Defenses based on sanitization or heuristic filtering often fail to catch these, especially in the presence of context sensitivity or long prompts. Certified-safety approaches have been proposed to mitigate this class: e.g., erase-and-check, which systematically removes tokens and reruns safety filters to detect adversarial prompt manipulations, providing (under assumptions) a safety guarantee against insertions, suffixes, or adversarial infusions up to a bounded size \cite{kumar2023certifying}. While promising, such methods are computationally expensive and may degrade user experience or model utility.

Because these perturbation-based attacks operate at the token level and can be obfuscated, they represent a difficult-to-detect threat, especially when combined with other attack modalities (multi-turn, universal triggers, trojans).

\section{Evaluation Metrics and Benchmarks}\label{sec:evaluation}

Robust and systematic evaluation is critical for red-teaming. The following metrics and benchmarks are widely adopted in research and practice.

\subsection{Quantitative Metrics}\label{sec:metrics-quant}

\begin{itemize}
  \item \textbf{Attack Success Rate (ASR):} the proportion of adversarial attempts (prompts) that succeed in eliciting harmful or policy-violating outputs. ASR is the most common metric, but it depends heavily on the definition of “harmful” and on the quality of the judge (classifier, LLM-judge, human).  
  \item \textbf{Refusal Bypass Rate:} a variant of ASR that measures how often safe-mode refusals or safety filters are bypassed — i.e., the model issues a disallowed output rather than refusing the request.  
  \item \textbf{Judge Reliability Metrics:} when using automated judges (regex, classifier, LLM-based), it is crucial to measure false positives / false negatives, calibration error, and inter-annotator agreement (if doing human calibration). Poor judges can substantially distort ASR and other metrics.  
  \item \textbf{Robustness Metrics:} measure how stable attacks (or defenses) are under variations — e.g., paraphrase of prompt, different model temperature, different seeds, context shuffling, or small perturbations.  
  \item \textbf{Transferability / Generality:} measure how well attacks discovered on one model or configuration transfer to other models, prompts, or deployments.  
  \item \textbf{Conversational / Trajectory Metrics:} in multi-turn attack scenarios, metrics may capture success over a dialogue trajectory (e.g., whether harmful content emerges at any point), time-to-failure, or complexity (number of turns needed).  
\end{itemize}

\subsection{Benchmarks and Standardized Suites}\label{sec:benchmarks}

To enable systematic comparison across red-teaming methods and LLMs, standard benchmarks have recently been developed. A prominent example is {\em HarmBench}, which provides an open-source evaluation framework, a large pool of red-teaming methods (18 methods) and a diverse set of target models and defenses (33 LLMs / defense settings) \cite{mazeika2024harmbench}. HarmBench enables reproducible large-scale red-team evaluation and supports both attack- and defense-side experiments. Its release marks a milestone toward standardizing LLM safety evaluation pipelines.

Despite this progress, many prior works still rely on ad-hoc prompt sets, non-public prompt libraries, or private internal red-team pipelines. In addition, while single-turn attack benchmarks are relatively common, multi-turn / conversational red-teaming remains underrepresented in publicly available benchmark suites.

\section{Open-source Red-Teaming Tools and Frameworks}\label{sec:tool-survey}

Several open-source frameworks and research prototypes aim to support systematic red-teaming. Their design choices, strengths, and limitations vary.

\subsection{Frameworks for rule-based or template-based red-teaming}\label{sec:pyrit-garak}

Frameworks such as {\em PyRIT} (discussed in the introduction) provide a modular architecture for plugging in model backends, defining prompt templates, logging results, and running interactive or batch red-team sessions. These tools are often lightweight and well-suited for smaller projects or smaller compute budgets, but tend to lack advanced automated search or generation capabilities.

Another such toolkit is {\em garak}, promoted by a major vendor and widely referenced in the industry. garak provides “probes, generators, and detectors”: probes manage attack logic; generators abstract target models (LLMs, dialog systems, or any component taking text and returning text); detectors assess whether output indicates a successful attack; and the framework compiles results into human-readable reports (HTML + JSON). This design allows red-teaming across a variety of model backends and output modalities, but — as with simpler frameworks — may not support automated, learning-based attack generation or multi-turn conversational scenarios out of the box \cite{turn0search32}.

\subsection{Automated Red-Teaming Frameworks}\label{sec:madmax-automatic}

More advanced frameworks attempt to integrate attack generation, execution, and evaluation into a unified pipeline. Notable recent work includes {\em MAD-MAX}, a modular adversarial red-teaming framework designed to allow multiple attack strategies (template-based, search-based, LLM-driven) in a pluggable architecture \cite{schoepf2025madmax}. Its modular nature makes it flexible and extensible, but in practice integrating it with diverse model runtimes (local LLaMA variants, API-based models, multi-modal models) and scaling up to large-scale red-teaming remains challenging due to compute cost, model compatibility, and evaluation infrastructure requirements.

The recent work on fully automated, trajectory-based red teaming — recasting red teaming as a sequential decision-making process over entire dialogues — pushes the frontier further. For example, the automated red-teaming approach by Belaire et al. (2025) formalizes multi-turn red teaming as a Markov Decision Process (MDP), enabling attack policies to optimize over entire conversation trajectories rather than single messages \cite{belaire2025automatic}. This methodology captures realistic adversarial behaviour and reveals vulnerabilities that single-turn or template-based attacks may miss.

\subsection{Defense-oriented and Certified Safety Approaches}\label{sec:defenses}

In response to the growing sophistication of attacks, some work focuses on hardening LLMs against adversarial prompting. For example, the framework “erase-and-check” provides a method for certifying safety against adversarial prompt modifications (suffix insertion, insertion at arbitrary positions, adversarial infusions) under bounded adversarial size. This method recomputes safety classification after systematically removing tokens from the prompt and offers provable safety guarantees under certain assumptions \cite{kumar2023certifying}. While promising, such approaches are often computationally expensive and may impair user experience or model usability.

Moreover, the existence of Trojan prompt attacks (as demonstrated by frameworks like {\em TrojLLM}) indicates that even seemingly benign prompts may embed stealthy triggers that cause harmful behaviour — which complicates defense strategies, necessitating robust input sanitization, runtime monitoring, and possibly dynamic prompt sanitization or adversarial-resistant prompt encoding \cite{xue2023trojllm}.

\section{Synthesis and Research Gap}\label{sec:gaps}

The literature and tools surveyed above show that — while the research community has developed a rich taxonomy of harms, attack strategies, and evaluation metrics — substantial gaps remain that limit the effectiveness, accessibility, and reliability of red-teaming for large language models. In particular:

\begin{itemize}
  \item \textbf{Lack of lightweight, modular, and extensible tooling:} existing frameworks either focus on small scale (template-based, rule-based, lightweight) or on large-scale automated red-teaming — but rarely both. There is a lack of toolkits that are accessible to small research teams or individual developers yet support modern LLM backends, extensible attack/evaluator plugins, and reproducible logging/benchmarks.  
  \item \textbf{Judge reliability and evaluation consistency:} many red-teaming efforts rely on heuristic or automated judges (regex filters, simple classifiers), which often lack calibration, robustness, or reproducibility. As a result, reported Attack Success Rates (ASR) may misrepresent true safety risk. While certified-safety methods (e.g., erase-and-check) provide stronger guarantees, they are computationally expensive and may be impractical for everyday red-teaming.  
  \item \textbf{Underrepresentation of multi-turn and real-world attack vectors:} most prior work and benchmarks focus on single-shot prompts; multi-turn conversational attacks — which more accurately model real-world adversaries — remain under-evaluated. Recently proposed trajectory-based red-teaming frameworks help, but are not yet part of standard open-source toolkits.  
  \item \textbf{Difficulty integrating across diverse model backends and deployment contexts:} LLMs are deployed in variable settings (local open-source models, API-based proprietary models, multi-modal agents, applications with external tools). Existing tools often lack abstractions or adapters covering this diversity.  
  \item \textbf{Limited transparency and reproducibility of prompt libraries and experiment artefacts:} many studies do not release their full prompt sets, seeds, or evaluation logs, making independent replication or longitudinal safety regression testing difficult.  
\end{itemize}

These gaps motivate the design goals for the toolkit developed in this thesis: namely, modularity and pluggability (attack generators, model adapters, evaluators), reproducible experiment manifests and logging, hybrid judging (to balance cost and fidelity), and explicit support for multi-turn conversational testing. By addressing these gaps, the proposed toolkit aims to lower the barrier to entry for systematic LLM red-teaming for researchers, students, and small teams.

\section{Conclusion of the Chapter}\label{sec:chapter-conclusion}

In this chapter we surveyed the technical foundations of large language models — their architecture, alignment techniques, and associated safety risks — and we reviewed a broad spectrum of attack vectors: from simple single-shot jailbreaks to prompt injection, multi-turn manipulations, universal triggers, and Trojan-style prompt attacks. We examined the diversity of red-teaming methodologies (manual, rule-based, and automated), and we discussed evaluation metrics and recent benchmark frameworks such as {\em HarmBench}. We also surveyed existing open-source tools and frameworks, highlighting trade-offs between accessibility, scalability, and coverage. Finally, we identified core gaps in tooling, evaluation, and reproducibility — gaps that motivate the design and implementation of the red-teaming toolkit proposed in the next chapter.

With this foundation in place, the next chapter will provide a detailed survey of existing open-source red-teaming toolkits, followed by a requirements analysis and the design of the proposed modular, extensible toolkit.

\begin{table}[htbp]
\centering
\footnotesize
\begin{tabularx}{\linewidth}{l X X X X X}
\toprule
\textbf{Framework} &
\textbf{Attack Types Supported} &
\textbf{Model Backend Support} &
\textbf{Multi-turn Capability} &
\textbf{Evaluation / Judge Type} &
\textbf{Key Strengths / Limitations} \\
\midrule

\textbf{PyRIT} \cite{munoz2024pyrit} &
Rule-based prompts, template attacks, basic prompt injection, simple adversarial transforms. &
Supports API-based models (OpenAI, Anthropic), simple REST adapters; limited support for local LLMs. &
\textcolor{gray}{Partial} – supports manual conversational sessions but lacks automated multi-turn logic. &
Basic keyword/regex checks; optional external classifiers. &
+ Lightweight, easy to use.  
-- Limited automation, no search-based generators, weak judge reliability. \\

\midrule
\textbf{garak} \cite{turn0search32} &
Template attacks, perturbation probes, obfuscation, prompt rewriting via “generators”. &
API models + HuggingFace backends; modular “generators” allow extension. &
\textcolor{gray}{Partial} – supports sequential probing, but not full conversational trajectories. &
Detectors: regex, classifiers; HTML/JSON reporting. &
+ Mature, widely used, structured probes.  
-- Limited advanced automated attack generation; multi-turn support minimal. \\

\midrule
\textbf{MAD-MAX} \cite{schoepf2025madmax} &
Modular system: rule-based, template-based, LLM-generated attacks, search-based attacks. &
Designed for flexible backends; supports API and local models; plugin-based adapters. &
\textbf{Yes} – modular conversation orchestrator; supports multi-turn flows. &
LLM-as-a-judge, rule-based detectors, modular evaluation pipeline. &
+ Highly modular, extensible.  
-- High compute requirements; complex to deploy; best suited for labs. \\

\midrule
\textbf{Automatic LLM Red Teaming} \\ (Belaire et al. 2025) \cite{belaire2025automatic} &
LLM-as-attacker, RL/HF-based optimisation, trajectory-level red teaming, multi-turn adversarial search. &
API and local models; depends on model-in-the-loop capabilities. &
\textbf{Yes} – multi-turn attacks optimized using RL / MDP formulation. &
LLM judges (safety classifiers, preference models). &
+ State-of-the-art automated discovery; finds subtle failures.  
-- Expensive; requires careful calibration; not lightweight. \\

\midrule
\textbf{TAP (Tree-of-Attacks)} \cite{mehrotra2023tap} &
Search-based automated jailbreaks, adversarial refinement tree, black-box optimization. &
Black-box API models (OpenAI, Anthropic, etc.). Local models possible with wrappers. &
\textcolor{gray}{No} – primarily single-turn; generates one-shot optimized attacks. &
Simple heuristic judges; optional LLM-as-a-judge extension. &
+ Extremely query-efficient; discovers novel jailbreaks.  
-- Limited to single-turn; no conversational capabilities; not a full toolkit. \\

\midrule
\textbf{Prompt Injection Frameworks} \\ (e.g., HouYi) \cite{liu2023promptinjectionapps} &
Prompt injection, indirect injection, document-based attacks, automated crawling and exploitation. &
Targets LLM-integrated apps (not just raw models). &
\textbf{Yes} – multi-step workflows across web/app interactions. &
Task-specific checks; manual validation often required. &
+ Highly realistic; identifies real-world vulnerabilities.  
-- Not general red-teaming against LLMs alone; focus on apps. \\

\midrule
\textbf{Certified Safety Tools} \\ (Erase-and-Check) \cite{kumar2023certifying} &
Defense: adversarial prompt removal, token deletion checks, suffix attack detection. &
Model-agnostic; operates at prompt-processing stage. &
N/A (defense, not attack). &
Binary safety guarantee after checking. &
+ Formal safety guarantees for bounded adversarial prompts.  
-- Very slow; not suitable for large-scale red-teaming. \\

\bottomrule
\end{tabularx}
\caption{Comparison of major LLM red-teaming frameworks, covering supported attack types, backend compatibility, multi-turn capabilities, evaluation methods, and key strengths and limitations.}
\label{tab:framework-comparison}
\end{table}


% For compilation piecewise (see projekt.tex), it is necessary to uncomment it
% \end{document}